# Speech-Emotion-Recognition-System-using-CNN-Bi-LSTM-
ğŸµ Speech Emotion Recognition using CNN + LSTM This project implements a deep learning pipeline for Speech Emotion Recognition (SER) using the CREMA-D dataset. The model combines Convolutional Neural Networks (CNNs) for extracting spatial features from spectrograms and LSTMs for capturing temporal dependencies in speech, enabling robust classification of human emotions from audio signals.

ğŸ“Œ Dataset: CREMA-D

Corpus of Emotional Multimodal Actors (CREMA-D)

7442 audio clips recorded by 91 actors (48 male, 43 female)

6 emotion categories:

ğŸ˜  Angry (ANG)

ğŸ˜¨ Fear (FEA)

ğŸ˜¢ Sad (SAD)

ğŸ˜ƒ Happy (HAP)

ğŸ˜ Neutral (NEU)

ğŸ¤¢ Disgust (DIS) Each audio file is labeled in the filename (e.g., 1079_DFA_ANG_XX.wav).

ğŸ”„ Pipeline

Data Preprocessing

Audio resampling to 16kHz mono

Noise reduction & silence trimming

Feature extraction:

MFCCs (40 coefficients + deltas)

Mel-Spectrograms (for CNN input)

Padding to fixed length

Model Architecture

CNN layers â†’ extract frequency & time-based features

LSTM layers â†’ capture sequential dependencies

Dense layers + Softmax â†’ classify into 6 emotions

Training Setup

Loss: categorical_crossentropy

Optimizer: Adam

Metrics: accuracy, f1-score

Regularization: Dropout, BatchNorm, EarlyStopping

Evaluation

Confusion Matrix

Per-class Precision, Recall, F1-score
